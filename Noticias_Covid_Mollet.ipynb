{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WebScrapping Mollet del Valles - Covid 19\n",
    "\n",
    "## Introducción\n",
    "\n",
    "La intención de esta notebook, es la de explicar un script python que cree al principio de la pandemia para recibir de forma automática (via crontab en un servidor linux que tengo en una jetson nano) las notícias locales (de mi pueblo) sobre covid recogidas en diferentes medios locales.\n",
    "\n",
    "## Medios Locales 'Scrapeados'\n",
    "\n",
    "- Listado de medios incluidos en el script\n",
    "\n",
    "[Nació Granollers](https://www.naciodigital.cat/naciogranollers/)\n",
    "\n",
    "[Som Mollet](https://www.sommollet.cat/)\n",
    "\n",
    "[El 9nou Vallès Oriental](https://el9nou.cat/valles-oriental/)\n",
    "\n",
    "[Mollet a Mà](https://www.clicama.cat/)\n",
    "\n",
    "[Ajuntament Mollet del Vallès](https://www.molletvalles.cat/)\n",
    "\n",
    "[Revista del Vallès](https://revistadelvalles.es/)\n",
    "\n",
    "## Pasos\n",
    "El script principalmente hace:\n",
    "- Scrapea el medio local con la dirección que me pareció mas fácil de scrapear.\n",
    "- Del contenido de las noticias busca 2 palabras clave, 'covid' 'mollet'.\n",
    "- Si se cumplen ambas añade el título de la notícia, parte del texto (se puede configurar) y el link de la notícia en un dataframe, que finalmente formateará como html y enviará por correo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import not_lib as nt #librería copiada (no encuentro la ref), paso al final\n",
    "import pandas as pd\n",
    "import clean_lib as cl #idem not_lib, sirven para dejar el texto legible\n",
    "from mail_lib import enviar_mail #librería propia, paso detalle al final\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creo el dataframe de 'trabajo' en el que exportaré las notícias al final y que enviaré por correo electrónico en formato 'html', no es necesario definirlo, pero a mi me ayuda a visualizar que datos utilizo y para que etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "noticias = pd.DataFrame(columns=['Titulo','Link','Texto','Origen'])\n",
    "pd.set_option('display.max_colwidth', None) #Con esto no truncamos el link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ahora iremos medio a medio informando el DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota** La url seleccionada, se ha realizado via ensayo y error (sin mucho sufrimiento), simplemente intentando buscar un punto de la web del medio que fuera facil conseguir las notícias y el link al contenido (busca primero todas las noticias de cabecera y después mira el contenido de cada una al completo)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### El 9nou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://el9nou.cat/valles-oriental/'\n",
    "soup = nt.f_get_data(url)\n",
    "art = soup.find(class_='noticies-portada')\n",
    "\n",
    "items = art.find_all('article')\n",
    "\n",
    "for i,item in enumerate(items):\n",
    "    url_item = item.find('a')['href']\n",
    "    tit_item = item.find('h1').getText()\n",
    "    soup2 = nt.f_get_data(url_item)\n",
    "    text_art = soup2.find(class_=\"col-md-12 marge\")\n",
    "    body = text_art.find_all('p')\n",
    "    texto = ''\n",
    "    origen = '9nou'\n",
    "    for body_item in body:\n",
    "        texto = texto + body_item.getText()\n",
    "    noticias.loc[len(noticias)]=[tit_item,url_item,texto,origen]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nació Granollers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "url2 = 'https://www.naciodigital.cat/naciogranollers/rss'\n",
    "request = urllib.request.Request(url2, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "response = urllib.request.urlopen(request, timeout=20)\n",
    "content = response.read()\n",
    "# Soup object (identify tags, etc..)\n",
    "soup = BeautifulSoup(content, \"lxml\")\n",
    "art = soup.find_all('item')\n",
    "for art_item in art:\n",
    "    title = art_item.find('title').getText()\n",
    "    link  = art_item.find('guid').getText()\n",
    "    soup2 = nt.f_get_data(link)\n",
    "    body = soup2.find_all(class_='amp_textnoticia')\n",
    "    texto = ''\n",
    "    origen = 'nacio'\n",
    "    for body_item in body:\n",
    "       texto = texto + body_item.getText()\n",
    "    noticias.loc[len(noticias)]=[title,link,texto,origen]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Som Mollet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "url3 = 'https://www.sommollet.cat/rss'\n",
    "request = urllib.request.Request(url3, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "response = urllib.request.urlopen(request, timeout=20)\n",
    "content = response.read()\n",
    "# Soup object (identify tags, etc..)\n",
    "soup = BeautifulSoup(content, \"lxml-xml\")\n",
    "art = soup.find_all('item')\n",
    "for art_item in art:\n",
    "    title = art_item.find('title').getText()\n",
    "    link  = art_item.find('guid').getText()\n",
    "    soup2 = nt.f_get_data(link)\n",
    "    body = soup2.find_all(class_='interior-main__content')\n",
    "    origen = 'sommollet'\n",
    "    texto = ''\n",
    "    for body_item in body:\n",
    "       texto = texto + body_item.getText()\n",
    "    noticias.loc[len(noticias)]=[title,link,texto,origen]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mollet a Mà"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "url4 = 'https://www.clicama.cat/rss'\n",
    "request = urllib.request.Request(url4, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "response = urllib.request.urlopen(request, timeout=20)\n",
    "content = response.read()\n",
    "# Soup object (identify tags, etc..)\n",
    "soup = BeautifulSoup(content, \"lxml-xml\")\n",
    "art = soup.find_all('item')\n",
    "for art_item in art:\n",
    "    title = art_item.find('title').getText()\n",
    "    link  = art_item.find('guid').getText()\n",
    "    soup2 = nt.f_get_data(link)\n",
    "    body = soup2.find_all(class_='content-data')\n",
    "    origen = 'molletama'\n",
    "    texto = ''\n",
    "    for body_item in body:\n",
    "       texto = texto + body_item.getText()\n",
    "    noticias.loc[len(noticias)]=[title,link,texto,origen]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ajuntament de Mollet del Vallès"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "url5 = 'https://www.molletvalles.cat/continguts-es-es/actualitat-es-es/notcies-es-es/'\n",
    "request = urllib.request.Request(url5, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "response = urllib.request.urlopen(request, timeout=20)\n",
    "content = response.read()\n",
    "# Soup object (identify tags, etc..)\n",
    "soup = BeautifulSoup(content, \"html.parser\")\n",
    "art = soup.find_all(class_='capsa_noticies_item')\n",
    "for i,art_item in enumerate(art):\n",
    "    link = art_item.find('a')['href']\n",
    "    origen = 'ajuntament'\n",
    "    if art_item.find('h3') != None:\n",
    "        title = art_item.find('h3').getText()\n",
    "        soup2 = nt.f_get_data(link)\n",
    "        body = soup2.find_all(style='text-align:justify')\n",
    "        texto = body[0].getText()\n",
    "        noticias.loc[len(noticias)]=[title,link,texto,origen]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Revista del Vallès"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "url6 = 'https://revistadelvalles.es/feed/'\n",
    "request = urllib.request.Request(url6, headers={'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_3) AppleWebKit/537.75.14 (KHTML, like Gecko) Version/7.0.3 Safari/7046A194A'})\n",
    "response = urllib.request.urlopen(request, timeout=20)\n",
    "content = response.read()\n",
    "# Soup object (identify tags, etc..)\n",
    "soup = BeautifulSoup(content, \"lxml-xml\")\n",
    "art = soup.find_all('item')\n",
    "for art_item in art:\n",
    "    title = art_item.find('title').getText()\n",
    "    link  = art_item.find('guid').getText()\n",
    "    soup2 = nt.f_get_data(link)\n",
    "    body = soup2.find_all(class_='td-post-content tagdiv-type')\n",
    "    origen = 'revistavalles'\n",
    "    texto = ''\n",
    "    for body_item in body:\n",
    "       texto = texto + body_item.getText()\n",
    "    noticias.loc[len(noticias)]=[title,link,texto,origen]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definimos la función de búsqueda (está en la librería not_lib, aquí pongo el detalle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def f_get_data(url):\n",
    "    #Import html information\n",
    "    request = urllib.request.Request(url,headers={'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_3) AppleWebKit/537.75.14 (KHTML, like Gecko) Version/7.0.3 Safari/7046A194A'})\n",
    "    response = urllib.request.urlopen(request, timeout=20)\n",
    "    content = response.read()\n",
    "    #Soup object (identify tgs, etc..)\n",
    "    soup = BeautifulSoup(content, \"html.parser\")\n",
    "    return soup\n",
    "\n",
    "def f_mollet_corona(text):\n",
    "    value = text.find('mollet')\n",
    "    value2 = text.find('coronavirus')\n",
    "    if value > 0 and value2 > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def busca_mollet(noticias):\n",
    "    encontrar = lambda x: f_mollet_corona(x)\n",
    "    noticias['Importante']=noticias.Clean.apply(encontrar)\n",
    "    noimportante = noticias[noticias['Importante']==0].index\n",
    "    noticias.drop(labels=noimportante,inplace=True)\n",
    "    return noticias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definimos la función de limpieza de texto etc... para facilitar la búsqueda (poner en minúsculas, quitar acentos, símbolos, etc..), está en la librería clean_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a first round of text cleaning techniques\n",
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text_round1(text):\n",
    "    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?¿\\]\\%', ' ', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "# Apply a second round of cleaning\n",
    "def clean_text_round2(text):\n",
    "    '''Get rid of some additional punctuation and non-sensical text that was missed the first time around.'''\n",
    "    text = re.sub('[‘’“”…«»]', '', text)\n",
    "    text = re.sub('\\n', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicamos las librerías de búsqueda y filtramos el conjunto de notícias que tenemos con las que nos interesan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Limpiamos\n",
    "round1 = lambda x: cl.clean_text_round1(x)\n",
    "noticias['Clean'] = noticias.Texto.apply(round1)\n",
    "round2 = lambda x: cl.clean_text_round2(x)\n",
    "noticias['Clean'] = noticias.Clean.apply(round2)\n",
    "\n",
    "resultado = nt.busca_mollet(noticias)\n",
    "resultado.drop(labels=['Texto','Clean','Importante','Origen'],axis=1,inplace=True)\n",
    "resultado.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finalmente Enviamos el df via mail, paso en primer termino las funciones que hay en mail_lib y después el código principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libreria para envio de correos html.\n",
    "import smtplib\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.base import MIMEBase\n",
    "from email import encoders\n",
    "\n",
    "def enviar_mail(direccion,df):\n",
    "    # Correo de acceso al servidor\n",
    "    MY_ADDRESS = 'mailuser'\n",
    "    # Password de acceso a la cuenta de email\n",
    "    PASSWORD = 'password'\n",
    "\n",
    "    # Configurar el servidor de correo\n",
    "    s = smtplib.SMTP(host='smtp.host.com', port=587) # servidor y puerto\n",
    "    s.starttls() # Conexion tls\n",
    "    s.login(MY_ADDRESS, PASSWORD) # Iniciar sesion con los datos de acceso al servidor SMTP\n",
    "\n",
    "    # Crear el Mensaje\n",
    "    msg = MIMEMultipart('alternative')\n",
    "    text = 'Hi All,\\n'\n",
    "    html = '''<h3>Últimas notícias sobre #coronavirus y #Mollet.</h3><p><p>\n",
    "    Las notícias pueden estar repetidas, se busca todas las que los medios tienen publicadas en sus paginas principales<p>{}<p><p>Sergi García<p>Python Analysis Tools'''.format(df.to_html())\n",
    "    # Record the MIME types of both parts - text/plain and text/html.\n",
    "    part1 = MIMEText(text, 'plain')\n",
    "    part2 = MIMEText(html, 'html')\n",
    "    # Attach parts into message container.\n",
    "    msg.attach(part1)\n",
    "    msg.attach(part2)\n",
    "\n",
    "    # Configurar los parametros del mensaje\n",
    "    msg['From']=MY_ADDRESS\n",
    "    msg['To']= direccion\n",
    "    msg['Subject']=\"Notícias Locales - #Coronaviurs #Mollet\"\n",
    "\n",
    "    # Enviar el mensaje\n",
    "    s.send_message(msg)\n",
    "    del msg\n",
    "\n",
    "    # Finaliar sesion SMTP\n",
    "    s.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enviar email\n",
    "to = 'mail@gmail.com'\n",
    "enviar_mail(to,resultado)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
